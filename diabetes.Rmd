---
title: "Detecting Diabetes - Case Study"
author: "Mariah Harvey"
date: "25 October 2016"
output:
  html_document:
    number_sections: true
    toc: true
    fig_width: 8
    fig_height: 5
    theme: readable
    highlight: tango
---
#Objective

The objective of this project is to correctly identify which patients show signs of diabetes according to the World Health Organization's criteria. In this project an outcome value equal to one is interpreted as "tested positive for diabetes", and zero otherwise. 

For this project two assumptions are made:
1) The sample was randomly drawn from the database.
2) Zeros indicate missing data in variables where a zero cannot be biologically possible (e.g. body mass index, blood pressure).

## Load and look at data
```{r, message = FALSE}
#Load Packages
library('mice') 
library('randomForest')
library("e1071")
library("caret")
```
```{r}
#Load Data
df<- read.csv("http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data", header=FALSE)
          
colnames(df)<-c("times_preg", "pgc", "dbp", "skin_fold", "insulin", "bmi", "diabetes_ped", "age", "class_var")
```
In order to get a general idea of the data we can look at the summary statistics and its structure
```{r}
str(df)
```
```{r}
summary(df)
```
# Multiple Imputation

The data includes zeros in places where zeros are biologically impossible, thus we can assume zeros are coded as missing data (except for times_pregnant, age, and class_var). Since we do not know if this data is missing completely at random (MCAR) we can use multiple imputation with chained equations to help us fill in these missing values.

## See which values are missing
```{r}
# Code zeros as na for all columns except times_preg, age, and class_var
is.na(df[,2:7]) <- !df[,2:7]

md.pattern(df)

```
From the above output we can tell that we are missing insulin data from 140 patients, and from 192 patients we are missing both skin fold and insulin data.  

## Impute and Examine Data
```{r}
imputed <- mice(df,m=5,maxit=50,method="pmm",seed=345)
summary(imputed)
complete_df<-complete(imputed)
```

This dataframe no longer has NA values. We should check the distribution of the insulin variable to make sure it remains similiar from before and after the imputation.

```{r}
par(mfrow=c(1,2))
hist(df$insulin, freq=F, main="Insulin with NAs", col="blue")
hist(complete_df$insulin, freq=F, main="Insulin without NAs", col="blue")

```

This looks okay. So let's go onto modeling.

#Prediction with Machine Learning 

To predict which patients are diagnosed with diabetes I am going to use two different supervised machine learning models and then evaluate them. 

##Split data into train and test groups 

In order to validate the prediction models used, I will split the dataset into a train and test set. The training set will be randomly made up of 80% of the sample, while the test set will be randomly made up of 20% of the sample.

```{r}
set.seed(209)
index_test <- sample(1:nrow(complete_df), size=0.2*nrow(complete_df))
test <- complete_df[index_test,]
train<-complete_df[-index_test,]

```

## Build a model using Random Forest
```{r}
set.seed(5)
rf_model <- randomForest(factor(class_var) ~ times_preg + pgc + dbp + skin_fold + insulin + bmi + diabetes_ped + age, data = train)
print(rf_model)
plot(rf_model, main="Error Rate Overall, Negative Diabetes, Positive Diabetes")
legend('bottomright', colnames(rf_model$err.rate), col=1:3, fill=1:3)
```

We can tell from the confusion matrix and the plot that our overall error rate is around 24%, whereas our error rate for predicting that a patient has diabetes is much higher (43%). Let's try to tune the algorithm useing the tuneRF command.

## Tune the Random Forest Model
```{r}
set.seed(8)
x <-train[,1:8]
y <-factor(train$class_var)
rf_tuned <- tuneRF(x=x, y=y, ntreeTry=500, stepFactor=2, improve=0.05, plot=TRUE)
print(rf_tuned)
```

From the above output we can tell that our best bet is to set mtry equal to two. 

```{r}
set.seed(4)
rf_model_tuned <- randomForest(factor(class_var) ~ ., data = train, mtry=2)
```

##Build a model using Support Vector Machines

```{r}
set.seed(20)
svm_model <- svm(factor(class_var) ~ times_preg + pgc + dbp + skin_fold + insulin + bmi + diabetes_ped + age, data = train)
summary(svm_model)
predict <- predict(svm_model,x)
table(predict,y)
```

## Tune the SVM
```{r}
set.seed(7)
svm_tune <- tune(svm, factor(class_var) ~., data=train, ranges=list(cost=10^(-1:2), gamma=c(.5,1,2)))
summary(svm_tune)
```

From the above tuning information we can determine the best parameters for cost and gamma.
```{r}
set.seed(4)
svm_model_tuned <- svm(factor(class_var) ~ ., data=train, kernel="radial", cost=1, gamma=0.5)
```

# Assess the Models
```{r}
# Random Forest
set.seed(1)
rf_test <- predict(rf_model_tuned, test)
confusionMatrix(factor(rf_test), factor(test$class_var))
```

```{r}
# SVM
set.seed(3)
svm_test <- predict(svm_model_tuned, test)
confusionMatrix(factor(svm_test), factor(test$class_var))
```

# Conclusion

From the above output we can tell that our Random Forest model had a slightly higher accuracy rate compared to our SVM model. In the future more models can be examined and tuned to hopefully increase accuracy. Additionally, feature engineering could be implemented to see if accuracy is improved. 




